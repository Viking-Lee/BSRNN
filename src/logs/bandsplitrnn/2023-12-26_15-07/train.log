[2023-12-26 15:07:47,858][__main__][INFO] - model:
  sr: 44100
  n_fft: 2048
  bandsplits:
  - - 1000
    - 100
  - - 4000
    - 250
  - - 8000
    - 500
  - - 16000
    - 1000
  - - 20000
    - 2000
  bottleneck_layer: rnn
  t_timesteps: 263
  fc_dim: 128
  rnn_dim: 256
  rnn_type: LSTM
  bidirectional: true
  num_layers: 12
  mlp_dim: 512
  return_mask: false
  complex_as_channel: true
  is_mono: ${..train_dataset.is_mono}
train_dataset:
  file_dir: D://dataset//MUSDB18HQ
  txt_dir: files/
  txt_path: null
  target: vocals
  is_training: true
  is_mono: false
  sr: 44100
  preload_dataset: false
  silent_prob: 0.1
  mix_prob: 0.25
  mix_tgt_too: false
val_dataset:
  file_dir: D://dataset//MUSDB18HQ
  txt_dir: files/
  txt_path: null
  target: ${..train_dataset.target}
  is_training: false
  is_mono: ${..train_dataset.is_mono}
  sr: ${..train_dataset.sr}
test_dataset:
  in_fp: D://dataset//MUSDB18HQ
  target: ${..train_dataset.target}
  is_mono: false
  sr: 44100
  win_size: 3
  hop_size: 0.5
  batch_size: 8
  window: null
sad:
  sr: 44100
  window_size_in_sec: 6
  overlap_ratio: 0.5
  n_chunks_per_segment: 10
  eps: 1.0e-05
  gamma: 0.001
  threshold_max_quantile: 0.15
  threshold_segment: 0.5
augmentations:
  randomcrop:
    _target_: data.augmentations.RandomCrop
    p: 1.0
    chunk_size_sec: 3
    sr: 44100
    window_stft: ${...featurizer.direct_transform.win_length}
    hop_stft: ${...featurizer.direct_transform.hop_length}
  gainscale:
    _target_: data.augmentations.GainScale
    p: 0.5
    min_db: -10.0
    max_db: 10.0
featurizer:
  direct_transform:
    _target_: torchaudio.transforms.Spectrogram
    n_fft: 2048
    win_length: 2048
    hop_length: 512
    power: null
  inverse_transform:
    _target_: torchaudio.transforms.InverseSpectrogram
    n_fft: 2048
    win_length: 2048
    hop_length: 512
callbacks:
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: epoch
  model_ckpt:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/loss
    mode: min
    save_top_k: 4
    dirpath: /weights
    filename: epoch{epoch:02d}-val_loss{val/loss:.2f}
    auto_insert_metric_name: false
  model_ckpt_usdr:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/usdr
    mode: max
    save_top_k: 4
    dirpath: ${..model_ckpt.dirpath}
    filename: epoch{epoch:02d}-val_usdr{val/usdr:.2f}
    auto_insert_metric_name: false
  early_stop:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val/loss
    mode: min
    patience: 10
    min_delta: 0.0001
  ema:
    _target_: utils.callbacks.EMA
    decay: 0.9999
    validate_original_weights: false
    every_n_steps: 1
train_loader:
  batch_size: 4
  num_workers: 8
  shuffle: true
  drop_last: true
val_loader:
  batch_size: 4
  num_workers: 8
  shuffle: false
  drop_last: false
opt:
  _target_: torch.optim.Adam
  lr: 0.001
sch:
  warmup_step: 10
  alpha: 0.1
  gamma: 0.98
ckpt_path: null
logger:
  _target_: pytorch_lightning.loggers.TensorBoardLogger
  save_dir: /tb_logs
  name: ''
  version: ''
  default_hp_metric: false
trainer:
  fast_dev_run: false
  max_epochs: 250
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 5
  log_every_n_steps: 100
  accelerator: auto
  devices: 1
  gradient_clip_val: 5
  precision: 32
  enable_progress_bar: true
  benchmark: true
  deterministic: false
experiment_dirname: bandsplitrnn

[2023-12-26 15:07:47,858][__main__][INFO] - Initializing loaders, featurizers.
